{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3f1d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 13:55:48.358030: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 13:55:48.358070: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#Load packages and modules\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be0984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "(X_train, Y_train),(X_valid, Y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d408fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "X_train = X_train.reshape(60000,784).astype('float32')\n",
    "X_valid = X_valid.reshape(10000,784).astype('float32')0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8819ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "X_train /=255\n",
    "X_valid /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "304b210b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
       "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
       "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
       "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
       "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
       "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
       "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
       "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
       "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
       "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a0d2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the labels to one hot representation\n",
    "from keras import utils as np_utils\n",
    "n_classes=10\n",
    "Y_train = keras.utils.np_utils.to_categorical(Y_train, n_classes)\n",
    "Y_valid = keras.utils.np_utils.to_categorical(Y_valid, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a0f7d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec377c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:01:42.333352: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-07 14:01:42.333395: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-07 14:01:42.333425: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterhub): /proc/driver/nvidia/version does not exist\n",
      "2022-11-07 14:01:42.370400: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Defining the model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b6b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding dense layer\n",
    "model.add(Dense(64, activation='sigmoid', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fc0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the final layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0843127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02038c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the network\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee30c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:06:49.315388: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "469/469 [==============================] - 4s 2ms/step - loss: 0.0928 - accuracy: 0.1013\n",
      "Epoch 2/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0918 - accuracy: 0.1040\n",
      "Epoch 3/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0911 - accuracy: 0.1117\n",
      "Epoch 4/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.1364\n",
      "Epoch 5/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0901 - accuracy: 0.1650\n",
      "Epoch 6/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0896 - accuracy: 0.1909\n",
      "Epoch 7/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0893 - accuracy: 0.2145\n",
      "Epoch 8/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0889 - accuracy: 0.2398\n",
      "Epoch 9/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0885 - accuracy: 0.2743\n",
      "Epoch 10/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.3132\n",
      "Epoch 11/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0879 - accuracy: 0.3506\n",
      "Epoch 12/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0876 - accuracy: 0.3821\n",
      "Epoch 13/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0873 - accuracy: 0.4051\n",
      "Epoch 14/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0870 - accuracy: 0.4220\n",
      "Epoch 15/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.4341\n",
      "Epoch 16/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0864 - accuracy: 0.4426\n",
      "Epoch 17/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.4520\n",
      "Epoch 18/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0857 - accuracy: 0.4590\n",
      "Epoch 19/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0854 - accuracy: 0.4647\n",
      "Epoch 20/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0851 - accuracy: 0.4696\n",
      "Epoch 21/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.4741\n",
      "Epoch 22/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.4770\n",
      "Epoch 23/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0841 - accuracy: 0.4799\n",
      "Epoch 24/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.4814\n",
      "Epoch 25/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0834 - accuracy: 0.4838\n",
      "Epoch 26/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0830 - accuracy: 0.4857\n",
      "Epoch 27/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.4871\n",
      "Epoch 28/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0823 - accuracy: 0.4885\n",
      "Epoch 29/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0819 - accuracy: 0.4899\n",
      "Epoch 30/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0815 - accuracy: 0.4925\n",
      "Epoch 31/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0811 - accuracy: 0.4932\n",
      "Epoch 32/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0807 - accuracy: 0.4953\n",
      "Epoch 33/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0803 - accuracy: 0.4966\n",
      "Epoch 34/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0798 - accuracy: 0.4991\n",
      "Epoch 35/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.5019\n",
      "Epoch 36/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0789 - accuracy: 0.5041\n",
      "Epoch 37/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0785 - accuracy: 0.5055\n",
      "Epoch 38/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.5085\n",
      "Epoch 39/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.5102\n",
      "Epoch 40/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0771 - accuracy: 0.5132\n",
      "Epoch 41/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.5154\n",
      "Epoch 42/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.5195\n",
      "Epoch 43/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.5220\n",
      "Epoch 44/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.5254\n",
      "Epoch 45/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.5293\n",
      "Epoch 46/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0741 - accuracy: 0.5324\n",
      "Epoch 47/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0736 - accuracy: 0.5354\n",
      "Epoch 48/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0731 - accuracy: 0.5386\n",
      "Epoch 49/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.5425\n",
      "Epoch 50/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0720 - accuracy: 0.5461\n",
      "Epoch 51/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0715 - accuracy: 0.5493\n",
      "Epoch 52/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0710 - accuracy: 0.5536\n",
      "Epoch 53/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0705 - accuracy: 0.5577\n",
      "Epoch 54/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0699 - accuracy: 0.5617\n",
      "Epoch 55/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0694 - accuracy: 0.5652\n",
      "Epoch 56/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0689 - accuracy: 0.5685\n",
      "Epoch 57/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.5727\n",
      "Epoch 58/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0678 - accuracy: 0.5776\n",
      "Epoch 59/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0673 - accuracy: 0.5819\n",
      "Epoch 60/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0668 - accuracy: 0.5859\n",
      "Epoch 61/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.5898\n",
      "Epoch 62/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0657 - accuracy: 0.5949\n",
      "Epoch 63/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0652 - accuracy: 0.5990\n",
      "Epoch 64/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0647 - accuracy: 0.6034\n",
      "Epoch 65/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0642 - accuracy: 0.6071\n",
      "Epoch 66/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0637 - accuracy: 0.6113\n",
      "Epoch 67/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0632 - accuracy: 0.6151\n",
      "Epoch 68/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0627 - accuracy: 0.6193\n",
      "Epoch 69/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0622 - accuracy: 0.6230\n",
      "Epoch 70/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0617 - accuracy: 0.6269\n",
      "Epoch 71/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0612 - accuracy: 0.6305\n",
      "Epoch 72/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0607 - accuracy: 0.6341\n",
      "Epoch 73/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0602 - accuracy: 0.6378\n",
      "Epoch 74/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0598 - accuracy: 0.6412\n",
      "Epoch 75/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0593 - accuracy: 0.6446\n",
      "Epoch 76/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0588 - accuracy: 0.6485\n",
      "Epoch 77/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0584 - accuracy: 0.6516\n",
      "Epoch 78/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0579 - accuracy: 0.6546\n",
      "Epoch 79/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0575 - accuracy: 0.6579\n",
      "Epoch 80/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0570 - accuracy: 0.6614\n",
      "Epoch 81/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0566 - accuracy: 0.6646\n",
      "Epoch 82/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0561 - accuracy: 0.6673\n",
      "Epoch 83/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.6700\n",
      "Epoch 84/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.6733\n",
      "Epoch 85/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0548 - accuracy: 0.6763\n",
      "Epoch 86/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.6792\n",
      "Epoch 87/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.6821\n",
      "Epoch 88/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.6855\n",
      "Epoch 89/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0532 - accuracy: 0.6881\n",
      "Epoch 90/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.6912\n",
      "Epoch 91/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.6939\n",
      "Epoch 92/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0520 - accuracy: 0.6969\n",
      "Epoch 93/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0516 - accuracy: 0.6997\n",
      "Epoch 94/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0512 - accuracy: 0.7027\n",
      "Epoch 95/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0508 - accuracy: 0.7055\n",
      "Epoch 96/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0505 - accuracy: 0.7085\n",
      "Epoch 97/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0501 - accuracy: 0.7111\n",
      "Epoch 98/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0497 - accuracy: 0.7144\n",
      "Epoch 99/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.7171\n",
      "Epoch 100/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0490 - accuracy: 0.7197\n",
      "Epoch 101/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0486 - accuracy: 0.7229\n",
      "Epoch 102/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0483 - accuracy: 0.7256\n",
      "Epoch 103/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0479 - accuracy: 0.7287\n",
      "Epoch 104/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0476 - accuracy: 0.7316\n",
      "Epoch 105/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0473 - accuracy: 0.7346\n",
      "Epoch 106/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0469 - accuracy: 0.7373\n",
      "Epoch 107/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0466 - accuracy: 0.7405\n",
      "Epoch 108/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0463 - accuracy: 0.7434\n",
      "Epoch 109/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0460 - accuracy: 0.7457\n",
      "Epoch 110/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0456 - accuracy: 0.7487\n",
      "Epoch 111/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0453 - accuracy: 0.7517\n",
      "Epoch 112/150\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0450 - accuracy: 0.7542\n",
      "Epoch 113/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0447 - accuracy: 0.7572\n",
      "Epoch 114/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0444 - accuracy: 0.7597\n",
      "Epoch 115/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0441 - accuracy: 0.7624\n",
      "Epoch 116/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.7650\n",
      "Epoch 117/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0435 - accuracy: 0.7678\n",
      "Epoch 118/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.7703\n",
      "Epoch 119/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0430 - accuracy: 0.7732\n",
      "Epoch 120/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.7760\n",
      "Epoch 121/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0424 - accuracy: 0.7777\n",
      "Epoch 122/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.7799\n",
      "Epoch 123/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.7828\n",
      "Epoch 124/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.7849\n",
      "Epoch 125/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.7874\n",
      "Epoch 126/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.7893\n",
      "Epoch 127/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0408 - accuracy: 0.7915\n",
      "Epoch 128/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.7931\n",
      "Epoch 129/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.7954\n",
      "Epoch 130/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.7972\n",
      "Epoch 131/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0398 - accuracy: 0.7992\n",
      "Epoch 132/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.8012\n",
      "Epoch 133/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0393 - accuracy: 0.8036\n",
      "Epoch 134/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.8049\n",
      "Epoch 135/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.8064\n",
      "Epoch 136/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.8077\n",
      "Epoch 137/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.8096\n",
      "Epoch 138/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.8107\n",
      "Epoch 139/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0379 - accuracy: 0.8129\n",
      "Epoch 140/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0377 - accuracy: 0.8140\n",
      "Epoch 141/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.8155\n",
      "Epoch 142/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.8168\n",
      "Epoch 143/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.8183\n",
      "Epoch 144/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.8199\n",
      "Epoch 145/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.8214\n",
      "Epoch 146/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0364 - accuracy: 0.8226\n",
      "Epoch 147/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.8238\n",
      "Epoch 148/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0360 - accuracy: 0.8248\n",
      "Epoch 149/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0358 - accuracy: 0.8263\n",
      "Epoch 150/150\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.8272\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6aebb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression(random_state=0))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, Y = fetch_openml(data_id=1464, return_X_y=True)\n",
    "X_train, X_valid, Y_train,Y_valid = train_test_split(X, Y, stratify=Y)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68b1bc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "Y_pred = clf.predict(X_valid)\n",
    "cm = confusion_matrix(Y_valid, Y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d0456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
